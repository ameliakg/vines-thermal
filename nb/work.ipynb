{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cee6e-98f2-4030-8868-d8ae9e1c33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import torch\n",
    "import skimage as ski\n",
    "import sklearn as skl\n",
    "\n",
    "from plantcv import plantcv as pcv\n",
    "import flyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74790dc-05d3-429d-bbdc-a1ae1ada0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a class\n",
    "class FlirDataset(torch.utils.data.Dataset):\n",
    "    \"\"\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, k):\n",
    "        return self.samples[k]\n",
    "    def __init__(self, path): \n",
    "        from glob import glob\n",
    "        from pathlib import Path\n",
    "        path = Path(path)\n",
    "        annot_pattern = str(path / \"training\" / \"annotated\" / \"*.png\")\n",
    "        annot_filenames = glob(annot_pattern)\n",
    "        annot_ims = {\n",
    "            Path(filename).name[:-4]: plt.imread(filename)\n",
    "            for filename in annot_filenames}\n",
    "        flir_pattern = str(path / \"*\" / \"thermal\" / \"*.jpg\")\n",
    "        flir_filenames = {\n",
    "            Path(file).name[:-4]: file\n",
    "            for file in glob(flir_pattern)}\n",
    "        self.names = list(annot_ims.keys())\n",
    "        flir_ims = {\n",
    "            key: self.load_flir(flir_filenames[key])\n",
    "            for key in self.names}\n",
    "        self.images = flir_ims\n",
    "        self.annots = annot_ims\n",
    "        #for (key, im, annot) in zip(ims.keys(), self.images, self.annot):\n",
    "        #    if im[1].shape[:2] != annot.shape[:2]:\n",
    "        #        print('!!!', key, im[0].shape, im[1].shape, annot.shape)\n",
    "        sdata = {}\n",
    "        for key in self.names:\n",
    "            (thr_im, opt_im) = self.images[key]\n",
    "            ann_im = self.annots[key]\n",
    "            for (rno, rowidx) in enumerate(range(0, opt_im.shape[0], 45)):\n",
    "                if rowidx + 45 >= opt_im.shape[0]:\n",
    "                    continue\n",
    "                for (cno, colidx) in enumerate(range(0, opt_im.shape[1], 45)):\n",
    "                    if colidx + 45 >= opt_im.shape[1]:\n",
    "                        continue\n",
    "                    # Get the subimage from the optical and annotation images:\n",
    "                    opt_sub = opt_im[rowidx:rowidx + 45, colidx:colidx + 45]\n",
    "                    thr_sub = opt_im[rowidx:rowidx + 45, colidx:colidx + 45]\n",
    "                    ann_sub = ann_im[rowidx:rowidx + 45, colidx:colidx + 45]\n",
    "                    tup = (rowidx, colidx, opt_sub, ann_sub, thr_sub)\n",
    "                    sdata[key, rno, cno] = tup\n",
    "        self.sample_data = sdata\n",
    "        self.masks = {}\n",
    "        self.samples = []\n",
    "        for ((k,rno,cno), tup) in sdata.items():\n",
    "            (rowidx, colidx, opt_sub, ann_sub, thr_sub) = tup\n",
    "            plant_pixels = np.all(ann_sub == [1, 0, 0, 1], axis=2)\n",
    "            self.masks[k, rno, cno] = plant_pixels\n",
    "            opt_for_torch = torch.permute(\n",
    "                torch.tensor(opt_sub, dtype=torch.float) / 255,\n",
    "                (2, 0, 1))\n",
    "            ann_frac = 1 - np.sum(plant_pixels) / plant_pixels.size\n",
    "            #ann_frac = torch.tensor(\n",
    "            #    round(ann_frac * 999),\n",
    "            #    dtype=torch.long)\n",
    "            ann_frac = torch.tensor(ann_frac, dtype=torch.float)\n",
    "            sample = (opt_for_torch, ann_frac)\n",
    "            self.samples.append(sample)\n",
    "    def load_flir(self, filename, thermal_unit='celsius'):\n",
    "        \"\"\"Loads and returns the portion of a FLIR image file that contains both\n",
    "        optical and thermal data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : pathlike\n",
    "            A ``pathname.Path`` object or a string representing the filename of\n",
    "            image that is to be loaded.\n",
    "        thermal_unit : {'celsius' | 'kelvin' | 'fahrenheit'}, optional\n",
    "            What temperature units to return; the default is ``'celsius'``.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        optical_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols, 3)`` containing the RGB\n",
    "            optical of the visual FLIR image.\n",
    "        thermal_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols)`` containing the thermal\n",
    "            values in Celsius.\n",
    "        \"\"\"\n",
    "        from os import fspath\n",
    "        from PIL import Image\n",
    "        import flyr\n",
    "        # Make sure we have a path:\n",
    "        filename = fspath(filename)\n",
    "        # Import the raw image data:\n",
    "        flir_image = flyr.unpack(filename)\n",
    "        # Extract the optical and thermal data:\n",
    "        opt = flir_image.optical\n",
    "        #plt.imshow(opt)\n",
    "        thr = getattr(flir_image, thermal_unit)\n",
    "        pip = flir_image.pip_info\n",
    "        x0 = pip.offset_x\n",
    "        y0 = pip.offset_y\n",
    "        ratio = pip.real_to_ir\n",
    "        ratio = opt.shape[0] / thr.shape[0] / ratio\n",
    "        # Resize the thermal image to match the optical image in resolution:\n",
    "        (opt_rs, opt_cs, _) = opt.shape\n",
    "        (thr_rs, thr_cs) = np.round(np.array(thr.shape) * ratio).astype(int)\n",
    "        thr = np.array(Image.fromarray(thr).resize([thr_cs, thr_rs]))\n",
    "        #plt.imshow(thr)\n",
    "        x0 = round(opt_cs // 2 - thr_cs // 2 + x0)\n",
    "        y0 = round(opt_rs // 2 - thr_rs // 2 + y0)\n",
    "        return (thr, opt[y0:y0+thr_rs, x0:x0+thr_cs, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57045441-be75-46e2-8877-330aa57be09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = FlirDataset(Path.home() / 'Desktop' / 'monthly images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97326088-a72d-4a86-b541-d42384c46854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlirResNet(torch.nn.Module):\n",
    "    def __init__(self, resnet='resnet18', weights='IMAGENET1K_V1'):\n",
    "        super().__init__()\n",
    "        self.resnet = torch.hub.load(\n",
    "            'pytorch/vision:v0.13.0', resnet, \n",
    "            weights=weights)\n",
    "        self.linear = torch.nn.Linear(1000, 1)\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(self.resnet(inputs)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f42f2-b574-45ba-b53b-da024e8d376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Hyperparameters:\n",
    "n_epochs = 8  # 1 epoch == show all training data to the model once.\n",
    "lr = 0.0005   # We use a fairly low learning rate.\n",
    "batch_size = len(train_ds)  # How many images in one training batch.\n",
    "\n",
    "# Make the model:\n",
    "model = FlirResNet()\n",
    "\n",
    "# Make the optimizer and LR-manager:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "steplr = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,\n",
    "    gamma=0.65)\n",
    "\n",
    "# Declare our loss function:\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# Make the dataloaders:\n",
    "train_dloader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "#test_dloader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now we start the optimization loop:\n",
    "for epoch_num in range(n_epochs):\n",
    "    # Put the model in train mode:\n",
    "    model.train()\n",
    "    # In each epoch, we go through each training sample once; the dataloader\n",
    "    # gives these to us in batches:\n",
    "    total_train_loss = 0\n",
    "    for (inputs, targets) in train_dloader:\n",
    "        # We're starting a new step, so we reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the model prediction for these inputs.\n",
    "        preds = model(inputs)\n",
    "        # Calculate the loss between the prediction and the actual outputs.\n",
    "        train_loss = loss_fn(torch.sigmoid(preds), targets)\n",
    "        # Have PyTorch backward-propagate the gradients.\n",
    "        train_loss.backward()\n",
    "        # Have the optimizer take a step:\n",
    "        optimizer.step()\n",
    "        # Add up the total training loss:\n",
    "        total_train_loss = total_train_loss + train_loss*len(targets)\n",
    "    # LR Scheduler step:\n",
    "    steplr.step()\n",
    "    mean_train_loss = total_train_loss.detach() / len(train_dset)\n",
    "    # Now that we've finished training, put the model back in evaluation mode.\n",
    "    #model.eval()\n",
    "    ## Evaluate the model using the test data.\n",
    "    #total_test_loss = 0\n",
    "    #for (inputs, targets) in test_dloader:\n",
    "    #    preds = model(inputs)\n",
    "    #    test_loss = loss_fn(preds, targets)\n",
    "    #    total_test_loss = total_test_loss + train_loss\n",
    "    #mean_test_loss = total_test_loss.detach() / len(test_dset)\n",
    "    # Print something about this step:\n",
    "    print(f\"Epoch {epoch_num:2d} loss: {mean_train_loss:6.3f}\")\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  train loss = \", float(mean_train_loss))\n",
    "#print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9ea3f-8f6a-4ca9-bc04-7af87768e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = train_dset\n",
    "\n",
    "x = []\n",
    "ims = []\n",
    "for (im,f) in ds:\n",
    "    x.append(f)\n",
    "    ims.append(im)\n",
    "ims = torch.stack(ims, 0)\n",
    "y = model(ims).flatten()\n",
    "y = torch.sigmoid(y)\n",
    "\n",
    "(x,y) = (np.array(x), y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f53200-bd67-4cb3-9a27-c09159b738e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b582cb-eab0-4499-858a-46ec8fa23990",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e8c8d-771b-445f-8696-73bfd838e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x,y) = np.transpose(xy)\n",
    "\n",
    "(fig,ax) = plt.subplots(1, 1, figsize=(5,4), dpi=288)\n",
    "\n",
    "ax.scatter(x*100, y*100, c='k', s=0.5, alpha=0.5)\n",
    "ax.plot([0,100],[0,100], 'r:', zorder=-10)\n",
    "ax.set_xlim([0,100])\n",
    "ax.set_ylim([0,100])\n",
    "ax.set_xlabel('True Plant Fraction [%]')\n",
    "ax.set_ylabel('Predicted Plant Fraction [%]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee1461-5eeb-47af-9772-b673947f7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cf13d-f517-48b1-a59b-57a3a16c1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "(im,f) = ds[134]\n",
    "print(f)\n",
    "plt.imshow(torch.permute(im, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb73b-305e-42f1-a0ec-66db1975ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = filenames[11]\n",
    "\n",
    "# (1) Read in the image:\n",
    "dat = flyr.unpack(filename)\n",
    "im0 = dat.optical\n",
    "th0 = dat.celsius\n",
    "im0 = np.flipud(np.transpose(im0, (1,0,2)))\n",
    "th0 = np.flipud(np.transpose(th0, (1,0)))\n",
    "\n",
    "# (2) Extract the yellow-blue channel:\n",
    "#im_b = im0[:,:,1] / np.mean(pcv.gaussian_blur(im0, (ksize, ksize)), axis=-1)\n",
    "#im_b /= 2\n",
    "#im_b = (np.clip(im_b, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "im_b = pcv.rgb2gray_lab(rgb_img=im0, channel='b')\n",
    "#im_b = pcv.hist_equalization(im_b)\n",
    "\n",
    "# (3) Pick a threshold:\n",
    "im_mask = pcv.threshold.binary(\n",
    "    gray_img=im_b,\n",
    "    threshold=130,\n",
    "    object_type='light')\n",
    "\n",
    "# (4) Delete the out-of-mask pieces of the original image.\n",
    "im_seg = np.array(im0)\n",
    "im_seg[im_mask > 0, :] = 255\n",
    "\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(7,7), dpi=288)\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].imshow(im0)\n",
    "axs[1].imshow(im_b, cmap='gray', vmin=0, vmax=255)\n",
    "axs[2].imshow(im_mask, cmap='gray', vmin=0, vmax=255)\n",
    "axs[3].imshow(im_seg, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda8049-3b7c-429c-b637-a5047c4f85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c7827-74a8-43c6-acb1-acd55007e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    mpl.image.imread('/Users/nben/Desktop/monthly images/6_25/RGB/FLIR2369.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bc588-d32f-4756-a448-de739ca8c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(th0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dc5fd-f02d-4a3f-b17e-428c3943081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00b96e-ab37-40e5-adfd-ca0672f078b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    np.stack(\n",
    "        [pcv.hist_equalization(im0[:,:,k])\n",
    "         for k in (0,1,2)],\n",
    "        axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25556d6f-1089-41a4-9d7d-9b3162a62e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pcv.threshold.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d5c94-a8d3-4263-9937-517d142957d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a081441-fe76-407b-ae38-0831565e7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_img = pcv.rgb2gray_lab(rgb_img=im, channel='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2b805-43aa-4218-bb66-bb071784b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(b_img, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b4719-3e37-4bed-bf89-c13b639a7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_mask = pcv.threshold.binary(gray_img=b_img, threshold=120, object_type='light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd272df-56c2-41b4-8489-4f0e7014fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thresh_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4c5d8-0912-488f-b5fa-40ef83ad1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "immask = np.array(im)\n",
    "immask[thresh_mask > 0, :] = 0\n",
    "plt.imshow(immask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc74a-f326-4006-91ff-e51b03c7f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_figure1, hist_data1 = pcv.visualize.histogram(img = b_img, hist_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056c237-8509-483f-b42d-61bb46bb270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_figure1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5882d9-ed4e-4f5e-b8ba-19e5b62124cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf6848-24e2-41a8-8113-4891d926c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = mpl.image.imread('/Users/nben/Desktop/FLIR3099.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839d93b-fce4-4a78-8972-9d9f6cd0f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
