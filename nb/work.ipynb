{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321cee6e-98f2-4030-8868-d8ae9e1c33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import skimage as ski\n",
    "import sklearn as skl\n",
    "\n",
    "from plantcv import plantcv as pcv\n",
    "import flyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74790dc-05d3-429d-bbdc-a1ae1ada0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader class\n",
    "class FlirDataset(torch.utils.data.Dataset):\n",
    "    \"\"\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, k):\n",
    "        return self.samples[k]\n",
    "    def __init__(self, path, train=True, image_subsize=64): \n",
    "        from glob import glob\n",
    "        from pathlib import Path\n",
    "        path = Path(path)\n",
    "        tt = \"train\" if train is True else \"test\" if train is False else \"none\" # TO DO: make a folder none with the cropped RBG images, leave thermal in monthly folders and then function will go and find them\n",
    "        annot_pattern = str(path / \"training\" / \"annotated\" / tt / \"*.png\")\n",
    "        annot_filenames = glob(annot_pattern)\n",
    "        annot_ims = {\n",
    "            Path(filename).name[:-4]: plt.imread(filename)\n",
    "            for filename in annot_filenames}\n",
    "        flir_pattern = str(path / \"*\" / \"thermal\" / \"*.jpg\")\n",
    "        flir_filenames = {\n",
    "            Path(file).name[:-4]: file\n",
    "            for file in glob(flir_pattern)}\n",
    "        self.names = list(annot_ims.keys())\n",
    "        flir_ims = {\n",
    "            key: self.load_flir(flir_filenames[key])\n",
    "            for key in self.names}\n",
    "        self.images = flir_ims\n",
    "        self.annots = annot_ims\n",
    "        #for (file, ims, annot) in zip(list(self.names()), list(self.images()), list(self.annots())):\n",
    "            #if ims[1].shape[:2] != annot.shape[:2]:\n",
    "               # plt.imshow(annot)\n",
    "                #print(file, ims[1].shape, annot.shape)\n",
    "        px = image_subsize       \n",
    "        sdata = {}\n",
    "        for key in self.names:\n",
    "            (thr_im, opt_im) = self.images[key]\n",
    "            ann_im = self.annots[key]\n",
    "            for (rno, rowidx) in enumerate(range(0, opt_im.shape[0], px)):\n",
    "                if rowidx + px >= opt_im.shape[0]:\n",
    "                    continue\n",
    "                for (cno, colidx) in enumerate(range(0, opt_im.shape[1], px)):\n",
    "                    if colidx + px >= opt_im.shape[1]:\n",
    "                        continue\n",
    "                    # Get the subimage from the optical and annotation images:\n",
    "                    opt_sub = opt_im[rowidx:rowidx + px, colidx:colidx + px]\n",
    "                    thr_sub = opt_im[rowidx:rowidx + px, colidx:colidx + px]\n",
    "                    ann_sub = ann_im[rowidx:rowidx + px, colidx:colidx + px]\n",
    "                    tup = (rowidx, colidx, opt_sub, ann_sub, thr_sub)\n",
    "                    sdata[key, rno, cno] = tup\n",
    "        self.sample_data = sdata\n",
    "        self.masks = {}\n",
    "        self.samples = []\n",
    "        for ((k,rno,cno), tup) in sdata.items():\n",
    "            (rowidx, colidx, opt_sub, ann_sub, thr_sub) = tup\n",
    "            plant_pixels = np.all(ann_sub == [1, 0, 0, 1], axis=2)\n",
    "            self.masks[k, rno, cno] = plant_pixels\n",
    "            opt_for_torch = torch.permute(\n",
    "                torch.tensor(opt_sub, dtype=torch.float) / 255,\n",
    "                (2, 0, 1))\n",
    "            ann_frac = 1 - np.sum(plant_pixels) / plant_pixels.size\n",
    "            #ann_frac = torch.tensor(\n",
    "            #    round(ann_frac * 999),\n",
    "            #    dtype=torch.long)\n",
    "            ann_frac = torch.tensor(ann_frac, dtype=torch.float)\n",
    "            #sample = (opt_for_torch, ann_frac)\n",
    "            mask = torch.tensor(self.masks[k, rno, cno], dtype=torch.float32)\n",
    "            sample = (opt_for_torch, mask[None,...])\n",
    "            self.samples.append(sample)\n",
    "    def load_flir(self, filename, thermal_unit='celsius'):\n",
    "        \"\"\"Loads and returns the portion of a FLIR image file that contains both\n",
    "        optical and thermal data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : pathlike\n",
    "            A ``pathname.Path`` object or a string representing the filename of\n",
    "            image that is to be loaded.\n",
    "        thermal_unit : {'celsius' | 'kelvin' | 'fahrenheit'}, optional\n",
    "            What temperature units to return; the default is ``'celsius'``.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        optical_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols, 3)`` containing the RGB\n",
    "            optical of the visual FLIR image.\n",
    "        thermal_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols)`` containing the thermal\n",
    "            values in Celsius.\n",
    "        \"\"\"\n",
    "        from os import fspath\n",
    "        from PIL import Image\n",
    "        import flyr\n",
    "        # Make sure we have a path:\n",
    "        filename = fspath(filename)\n",
    "        # Import the raw image data:\n",
    "        flir_image = flyr.unpack(filename)\n",
    "        # Extract the optical and thermal data:\n",
    "        opt = flir_image.optical\n",
    "        #plt.imshow(opt)\n",
    "        thr = getattr(flir_image, thermal_unit)\n",
    "        pip = flir_image.pip_info\n",
    "        x0 = pip.offset_x\n",
    "        y0 = pip.offset_y\n",
    "        ratio = pip.real_to_ir\n",
    "        ratio = opt.shape[0] / thr.shape[0] / ratio\n",
    "        # Resize the thermal image to match the optical image in resolution:\n",
    "        (opt_rs, opt_cs, _) = opt.shape\n",
    "        (thr_rs, thr_cs) = np.round(np.array(thr.shape) * ratio).astype(int)\n",
    "        thr = np.array(Image.fromarray(thr).resize([thr_cs, thr_rs]))\n",
    "        #plt.imshow(thr)\n",
    "        x0 = round(opt_cs // 2 - thr_cs // 2 + x0)\n",
    "        y0 = round(opt_rs // 2 - thr_rs // 2 + y0)\n",
    "        return (thr, opt[y0:y0+thr_rs, x0:x0+thr_cs, :])\n",
    "\n",
    "\n",
    "    def extract_temp(self, model):\n",
    "        \"\"\"Extracts temperature from predicted plant segmentation\"\"\"\n",
    "        results = []\n",
    "        for (input, target) in self.samples:\n",
    "            pred = model(input[None,...]) #need none bc model expecting batch, so gives it a batch dimension\n",
    "            pred = torch.sigmoid(pred) # gives probabilities\n",
    "            pred = pred > 0.5\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f438804-63c3-44f7-876c-df4957d6c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making a training and testing dataset\n",
    "# from pathlib import Path\n",
    "# p = Path(\"/Users/ameliakeyser-gibson/Documents/SEFS/Kim Lab/vines grant/thermal/monthly images/training/annotated\")\n",
    "# files = list(p.iterdir())\n",
    "# test = np.random.choice(files, round(len(files)*0.2))\n",
    "# \" \".join([f.name for f in test if \".png\" in f.name])\n",
    "## used mv in terminal to move file string to new folder for train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57045441-be75-46e2-8877-330aa57be09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name the datasets\n",
    "train_ds = FlirDataset(\"/Users/ameliakeyser-gibson/Documents/SEFS/Kim Lab/vines grant/thermal/monthly images\", train = True)\n",
    "test_ds = FlirDataset(\"/Users/ameliakeyser-gibson/Documents/SEFS/Kim Lab/vines grant/thermal/monthly images\", train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585a84e1-c7df-43cb-a579-5da48fa9bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f6cafb-2947-4e1f-bd68-f1ba58aada24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97326088-a72d-4a86-b541-d42384c46854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FlirResNet Code\n",
    "#class FlirResNet(torch.nn.Module):\n",
    "    #def __init__(self, resnet='resnet18', weights='IMAGENET1K_V1'):\n",
    "        #super().__init__()\n",
    "        #self.resnet = torch.hub.load(\n",
    "            #'pytorch/vision:v0.13.0', resnet, \n",
    "            #weights=weights)\n",
    "        #self.linear = torch.nn.Linear(1000, 1)\n",
    "    #def forward(self, inputs):\n",
    "        #return self.linear(self.resnet(inputs)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c02c52c-ceb0-4823-aab1-ace7ba5e19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "#U-net code\n",
    "# Dependencies\n",
    "def convrelu(in_channels, out_channels,\n",
    "             kernel=3, padding=None, stride=1, bias=True, inplace=True):\n",
    "    \"\"\"Shortcut for creating a PyTorch 2D convolution followed by a ReLU.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        The number of input channels in the convolution.\n",
    "    out_channels : int\n",
    "        The number of output channels in the convolution.\n",
    "    kernel : int, optional\n",
    "        The kernel size for the convolution (default: 3).\n",
    "    padding : int or None, optional\n",
    "        The padding size for the convolution; if `None` (the default), then\n",
    "        chooses a padding size that attempts to maintain the image-size.\n",
    "    stride : int, optional\n",
    "        The stride to use in the convolution (default: 1).\n",
    "    bias : boolean, optional\n",
    "        Whether the convolution has a learnable bias (default: True).\n",
    "    inplace : boolean, optional\n",
    "        Whether to perform the ReLU operation in-place (default: True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Sequential\n",
    "        The model of a 2D-convolution followed by a ReLU operation.\n",
    "    \"\"\"\n",
    "    if padding is None:\n",
    "        padding = kernel_default_padding(kernel)\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, out_channels, kernel,\n",
    "                        padding=padding, bias=bias),\n",
    "        torch.nn.ReLU(inplace=inplace))\n",
    "\n",
    "#===============================================================================\n",
    "# Image-based CNN Model Code\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    \"\"\"a U-Net with a ResNet18 backbone for learning visual area labels.\n",
    "\n",
    "    The `UNet` class implements a [\"U-Net\"](https://arxiv.org/abs/1505.04597)\n",
    "    with a [ResNet-18](https://pytorch.org/hub/pytorch_vision_resnet/) bacbone.\n",
    "    The class inherits from `torch.nn.Module`.\n",
    "    \n",
    "    The original implementation of this class was by Shaoling Chen\n",
    "    (sc6995@nyu.edu), and additional modifications have been made by Noah C.\n",
    "    Benson (nben@uw.edu).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_count : int\n",
    "        The number of channels (features) in the input image. When using an\n",
    "        `HCPVisualDataset` object for training, this value should be set to 4\n",
    "        if the dataset uses the `'anat'` or `'func'` features and 8 if it uses\n",
    "        the `'both'` features.\n",
    "    segment_count : int\n",
    "        The number of segments (AKA classes, labels) in the output data. For\n",
    "        V1-V3 this is typically either 3 (V1, V2, V3) or 6 (LV1, LV2, LV3, RV1,\n",
    "        RV2, RV3).\n",
    "    base_model : model name or tuple, optional\n",
    "        The name of the model that is to be used as the base/backbone of the\n",
    "        UNet. The default is `'resnet18'`, but \n",
    "    pretrained : boolean, optional\n",
    "        Whether to use a pretrained base model for the backbone (`True`) or not\n",
    "        (`False`). The default is `False`.\n",
    "    logits : boolean, optional\n",
    "        Whether the model should return logits (`True`) or probabilities\n",
    "        (`False`). The default is `True`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pretrained_base : boolean\n",
    "        `True` if the base model used in this `UNet` was originally pre-trained\n",
    "        and `False` otherwise.\n",
    "    base_model : PyTorch Module\n",
    "        The ResNet-18 model that is used as the backbone of the `UNet` model.\n",
    "    base_layers : list of PyTorch Modules\n",
    "        The ResNet-18 layers that are used in the backbone of the `UNet` model.\n",
    "    feature_count : int\n",
    "        The number of input channels (features) that the model expects in input\n",
    "        images.\n",
    "    segment_count : int\n",
    "        The number of segments (labels) predicted by the model.\n",
    "    logits : bool\n",
    "        `True` if the output of the model is in logits and `False` if its output\n",
    "        is in probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_count=3, segment_count=1,\n",
    "                 base_model='resnet18',\n",
    "                 pretrained=True,\n",
    "                 logits=True):\n",
    "        import torch.nn as nn\n",
    "        # Initialize the super-class.\n",
    "        super().__init__()\n",
    "        # Store some basic attributes.\n",
    "        self.feature_count = feature_count\n",
    "        self.segment_count = segment_count\n",
    "        self.pretrained = pretrained\n",
    "        self.logits = logits\n",
    "        # Set up the base model and base layers for the model.\n",
    "        if pretrained:\n",
    "            weights = 'IMAGENET1K_V1'\n",
    "        else:\n",
    "            weights = None\n",
    "        import torchvision.models as mdls\n",
    "        base_model = getattr(mdls, base_model)\n",
    "        try:\n",
    "            base_model = base_model(weights=weights)\n",
    "        except TypeError:\n",
    "            base_model = base_model(pretrained=pretrained)\n",
    "        # Not sure we should store the base model; seems like a good idea, but\n",
    "        # does it get caught up in PyTorch's Module data when we do?\n",
    "        #self.base_model = resnet18(pretrained=pretrained)\n",
    "        # Because the input size may not be 3 and the output size may not be 3,\n",
    "        # we want to add an additional \n",
    "        if feature_count != 3:\n",
    "            # Adjust the first convolution's number of input channels.\n",
    "            c1 = base_model.conv1\n",
    "            base_model.conv1 = nn.Conv2d(\n",
    "                feature_count, c1.out_channels,\n",
    "                kernel_size=c1.kernel_size, stride=c1.stride,\n",
    "                padding=c1.padding, bias=c1.bias)\n",
    "        base_layers = list(base_model.children())\n",
    "        #self.base_layers = base_layers\n",
    "        # Make the U-Net layers out of the base-layers.\n",
    "        # size = (N, 64, H/2, W/2)\n",
    "        self.layer0 = nn.Sequential(*base_layers[:3]) \n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 64, H/4, W/4)\n",
    "        self.layer1 = nn.Sequential(*base_layers[3:5])\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 128, H/8, W/8)        \n",
    "        self.layer2 = base_layers[5]\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        # size = (N, 256, H/16, W/16)\n",
    "        self.layer3 = base_layers[6]  \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        # size = (N, 512, H/32, W/32)\n",
    "        self.layer4 = base_layers[7]\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "        # The up-swing of the UNet; we will need to upsample the image.\n",
    "        self.upsample = nn.Upsample(scale_factor=2,\n",
    "                                    mode='bilinear',\n",
    "                                    align_corners=True)\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        self.conv_original_size0 = convrelu(feature_count, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        self.conv_last = nn.Conv2d(64, segment_count, 1)\n",
    "    def forward(self, input):\n",
    "        # Do the original size convolutions.\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        # Now the front few layers, which we save for adding back in on the UNet\n",
    "        # up-swing below.\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        # Now, we start the up-swing; each step must upsample the image.\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        # Up-swing Step 1\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        # Up-swing Step 2\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        # Up-swing Step 3\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "        # Up-swing Step 4\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        # Up-swing Step 5\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        # And the final convolution.\n",
    "        out = self.conv_last(x)\n",
    "        if not self.logits:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f4c7e-f9cd-4eba-a017-9c265d55035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #unet training without testing                                                                                                                                                                                                                                                                                                    # without testing\n",
    "# import torch\n",
    "# import torchvision\n",
    "\n",
    "# # Hyperparameters:\n",
    "# n_epochs = 8  # 1 epoch == show all training data to the model once.\n",
    "# lr = 0.0005   # We use a fairly low learning rate.\n",
    "# batch_size = len(train_ds)  # How many images in one training batch.\n",
    "\n",
    "# # Make the model:\n",
    "# model = UNet()\n",
    "\n",
    "# # Make the optimizer and LR-manager:\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# steplr = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=1,\n",
    "#     gamma=0.65)\n",
    "\n",
    "# # Declare our loss function:\n",
    "# loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# # Make the dataloaders:\n",
    "# train_dloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "# #test_dloader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Now we start the optimization loop:\n",
    "# for epoch_num in range(n_epochs):\n",
    "#     # Put the model in train mode:\n",
    "#     model.train()\n",
    "#     # In each epoch, we go through each training sample once; the dataloader\n",
    "#     # gives these to us in batches:\n",
    "#     total_train_loss = 0\n",
    "#     for (inputs, targets) in train_dloader:\n",
    "#         # We're starting a new step, so we reset the gradients.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculate the model prediction for these inputs.\n",
    "#         preds = model(inputs)\n",
    "#         # Calculate the loss between the prediction and the actual outputs.\n",
    "#         train_loss = loss_fn(torch.sigmoid(preds), targets)\n",
    "#         # Have PyTorch backward-propagate the gradients.\n",
    "#         train_loss.backward()\n",
    "#         # Have the optimizer take a step:\n",
    "#         optimizer.step()\n",
    "#         # Add up the total training loss:\n",
    "#         total_train_loss = total_train_loss + train_loss*len(targets)\n",
    "#     # LR Scheduler step:\n",
    "#     steplr.step()\n",
    "#     mean_train_loss = total_train_loss.detach() / len(train_ds)\n",
    "#     # Now that we've finished training, put the model back in evaluation mode.\n",
    "#     #model.eval()\n",
    "#     ## Evaluate the model using the test data.\n",
    "#     #total_test_loss = 0\n",
    "#     #for (inputs, targets) in test_dloader:\n",
    "#     #    preds = model(inputs)\n",
    "#     #    test_loss = loss_fn(preds, targets)\n",
    "#     #    total_test_loss = total_test_loss + test_loss\n",
    "#     #mean_test_loss = total_test_loss.detach() / len(test_dset)\n",
    "#     # Print something about this step:\n",
    "#     print(f\"Epoch {epoch_num:2d} loss: {mean_train_loss:6.3f}\")\n",
    "# # After the optimizer has run, print out what it's found:\n",
    "# print(\"Final result:\")\n",
    "# print(f\"  train loss = \", float(mean_train_loss))\n",
    "# #print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "617a0719-676f-4271-8279-41c85694c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 loss:  0.443\n",
      "Epoch  1 loss:  0.290\n",
      "Epoch  2 loss:  0.276\n",
      "Epoch  3 loss:  0.289\n",
      "Epoch  4 loss:  0.262\n",
      "Epoch  5 loss:  0.246\n",
      "Epoch  6 loss:  0.240\n",
      "Epoch  7 loss:  0.223\n",
      "Final result:\n",
      "  train loss =  0.2231207638978958\n",
      "   test loss =  0.16274765133857727\n"
     ]
    }
   ],
   "source": [
    "# U-net with testing\n",
    "#import torch\n",
    "#import torchvision\n",
    "\n",
    "# Hyperparameters:\n",
    "n_epochs = 8  # 1 epoch == show all training data to the model once. 8 is a small number to start with\n",
    "lr = 0.001   # We use a fairly low learning rate. take small steps as you train\n",
    "batch_size = 8  # How many images in one training batch. \n",
    "\n",
    "# Make the model:\n",
    "model = UNet()\n",
    "\n",
    "# Make the optimizer and LR-manager:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer handles updating the parameters each step\n",
    "steplr = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,\n",
    "    gamma=0.85) #take smaller steps in the learning rate as you get closer\n",
    "\n",
    "# Declare our loss function: what's actually getting minimized\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()  #loss function that works for pixels and logits- well established\n",
    "\n",
    "# Make the dataloaders:\n",
    "train_dloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)\n",
    "\n",
    "# Now we start the optimization loop:\n",
    "for epoch_num in range(n_epochs):\n",
    "    # Put the model in train mode:\n",
    "    model.train()\n",
    "    # In each epoch, we go through each training sample once; the dataloader\n",
    "    # gives these to us in batches:\n",
    "    total_train_loss = 0\n",
    "    for (inputs, targets) in train_dloader:\n",
    "        # We're starting a new step, so we reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the model prediction for these inputs.\n",
    "        preds = model(inputs)\n",
    "        # Calculate the loss between the prediction and the actual outputs.\n",
    "        train_loss = loss_fn(preds, targets) #sigmoid gives the probability but don't need sigmoid bc of BCE w/logit loss\n",
    "        # Have PyTorch backward-propagate the gradients.\n",
    "        train_loss.backward()\n",
    "        # Have the optimizer take a step: (update the parameters)\n",
    "        optimizer.step()\n",
    "        # Add up the total training loss:\n",
    "        total_train_loss = total_train_loss + train_loss*len(targets)\n",
    "    # LR Scheduler step:\n",
    "    steplr.step() #make the learning rate smaller\n",
    "    mean_train_loss = total_train_loss.detach() / len(train_ds)\n",
    "    # Now that we've finished training, put the model back in evaluation mode.\n",
    "    model.eval()\n",
    "    ## Evaluate the model using the test data.\n",
    "    total_test_loss = 0\n",
    "    for (inputs, targets) in test_dloader:\n",
    "        preds = model(inputs)\n",
    "        test_loss = loss_fn(preds, targets)\n",
    "        total_test_loss = total_test_loss + test_loss*len(targets) # changed from train loss\n",
    "    mean_test_loss = total_test_loss.detach() / len(test_ds)\n",
    "    # Print something about this step:\n",
    "    print(f\"Epoch {epoch_num:2d} loss: {mean_train_loss:6.3f}\")\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  train loss = \", float(mean_train_loss))\n",
    "print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f42f2-b574-45ba-b53b-da024e8d376b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # training/testing for the resnet\n",
    "# import torch\n",
    "# import torchvision\n",
    "\n",
    "# # Hyperparameters:\n",
    "# n_epochs = 15  # 1 epoch == show all training data to the model once. 8 is a small number to start with\n",
    "# lr = 0.0005   # We use a fairly low learning rate. take small steps as you train\n",
    "# batch_size = len(train_ds)  # How many images in one training batch. \n",
    "\n",
    "# # Make the model:\n",
    "# model = FlirResNet()\n",
    "\n",
    "# # Make the optimizer and LR-manager:\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer handles updating the parameters each step\n",
    "# steplr = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=1,\n",
    "#     gamma=0.65) #take smaller steps in the learning rate as you get closer\n",
    "\n",
    "# # Declare our loss function: what's actually getting minimized\n",
    "# loss_fn = torch.nn.L1Loss() \n",
    "\n",
    "# # Make the dataloaders:\n",
    "# train_dloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "# test_dloader = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)\n",
    "\n",
    "# # Now we start the optimization loop:\n",
    "# for epoch_num in range(n_epochs):\n",
    "#     # Put the model in train mode:\n",
    "#     model.train()\n",
    "#     # In each epoch, we go through each training sample once; the dataloader\n",
    "#     # gives these to us in batches:\n",
    "#     total_train_loss = 0\n",
    "#     for (inputs, targets) in train_dloader:\n",
    "#         # We're starting a new step, so we reset the gradients.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculate the model prediction for these inputs.\n",
    "#         preds = model(inputs)\n",
    "#         # Calculate the loss between the prediction and the actual outputs.\n",
    "#         train_loss = loss_fn(torch.sigmoid(preds), targets) #sigmoid gives the probability\n",
    "#         # Have PyTorch backward-propagate the gradients.\n",
    "#         train_loss.backward()\n",
    "#         # Have the optimizer take a step: (update the parameters)\n",
    "#         optimizer.step()\n",
    "#         # Add up the total training loss:\n",
    "#         total_train_loss = total_train_loss + train_loss*len(targets)\n",
    "#     # LR Scheduler step:\n",
    "#     steplr.step() #make the learning rate smaller\n",
    "#     mean_train_loss = total_train_loss.detach() / len(train_ds)\n",
    "#     # Now that we've finished training, put the model back in evaluation mode.\n",
    "#     model.eval()\n",
    "#     ## Evaluate the model using the test data.\n",
    "#     total_test_loss = 0\n",
    "#     for (inputs, targets) in test_dloader:\n",
    "#         preds = model(inputs)\n",
    "#         test_loss = loss_fn(preds, targets)\n",
    "#         total_test_loss = total_test_loss + test_loss # changed from train loss\n",
    "#     mean_test_loss = total_test_loss.detach() / len(test_ds)\n",
    "#     # Print something about this step:\n",
    "#     print(f\"Epoch {epoch_num:2d} loss: {mean_train_loss:6.3f}\")\n",
    "# # After the optimizer has run, print out what it's found:\n",
    "# print(\"Final result:\")\n",
    "# print(f\"  train loss = \", float(mean_train_loss))\n",
    "# print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5e27bb9-d117-48fa-89e7-40221f9e0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function used in dice coefficient function\n",
    "def is_logits(data): \n",
    "    \"\"\"Attempts to guess whether the given PyTorch tensor contains logits.\n",
    "\n",
    "    If the argument `data` contains only values that are no less than 0 and no\n",
    "    greater than 1, then `False` is returned; otherwise, `True` is returned.\n",
    "    \"\"\"\n",
    "    if   (data > 1).any(): return True\n",
    "    elif (data < 0).any(): return True\n",
    "    else:                  return False\n",
    "# writing a test DICE loss coefficient        \n",
    "def dice_loss(pred, gold, logits=None, smoothing=0, metrics=None):\n",
    "    \"\"\"Returns the loss based on the dice coefficient.\n",
    "    \n",
    "    `dice_loss(pred, gold)` returns the dice-coefficient loss between the\n",
    "    tensors `pred` and `gold` which must be the same shape and which should\n",
    "    represent probabilities. The first two dimensions of both `pred` and `gold`\n",
    "    must represent the batch-size and the classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : tensor\n",
    "        The predicted probabilities of each class.\n",
    "    gold : tensor\n",
    "        The gold-standard labels for each class.\n",
    "    logits : boolean, optional\n",
    "        Whether the values in `pred` are logits--i.e., unnormalized scores that\n",
    "        have not been run through a sigmoid calculation already. If this is\n",
    "        `True`, then the BCE starts by calculating the sigmoid of the `pred`\n",
    "        argument. If `None`, then attempts to deduce whether the input is or is\n",
    "        not logits. The default is `None`.\n",
    "    smoothing : number, optional\n",
    "        The smoothing coefficient `s`. The default is `1`.\n",
    "    metrics : dict or None, optional\n",
    "        An optional dictionary into which the key `'dice'` should be inserted\n",
    "        with the dice-loss as the value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The dice-coefficient loss of the prediction.\n",
    "    \"\"\"\n",
    "    pred = pred.contiguous()\n",
    "    gold = gold.contiguous()\n",
    "    if logits is None: logits = is_logits(pred) #sometimes logit, sometimes probability, this func automatically detect whether logit or prob\n",
    "    if logits: pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * gold) #high probabilities get higher values, low get low, gold is 0s and 1s, this gives predicted probabilities where true value is correct\n",
    "    pred = pred**2 #noah checking if we should be squaring here\n",
    "    gold = gold**2\n",
    "    while len(intersection.shape) > 2:\n",
    "        intersection = intersection.sum(dim=-1)\n",
    "        pred = pred.sum(dim=-1)\n",
    "        gold = gold.sum(dim=-1)\n",
    "    if smoothing is None: smoothing = 0\n",
    "    loss = (1 - ((2 * intersection + smoothing) / (pred + gold + smoothing)))\n",
    "    # Average the loss across classes then take the mean across batch elements.\n",
    "    loss = loss.mean(dim=1).mean() #utilities that we can ignore - mean across channels and then across all the batches\n",
    "    if metrics is not None:\n",
    "        if 'dice' not in metrics: metrics['dice'] = 0.0\n",
    "        metrics['dice'] += loss.data.cpu().numpy() * gold.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f690392c-96e3-40c1-9542-9dc96ad4ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 loss:  0.223\n",
      "Final result:\n",
      "  train loss =  0.2231207638978958\n",
      "   test loss =  0.5883352756500244\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "## Evaluate the model using the test data.\n",
    "total_test_loss = 0\n",
    "for (inputs, targets) in test_dloader:\n",
    "    preds = model(inputs)\n",
    "    test_loss = dice_loss(preds, targets, logits = True)\n",
    "    total_test_loss = total_test_loss + test_loss*len(targets) # changed from train loss\n",
    "mean_test_loss = total_test_loss.detach() / len(test_ds)\n",
    "# Print something about this step:\n",
    "print(f\"Epoch {epoch_num:2d} loss: {mean_train_loss:6.3f}\")\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  train loss = \", float(mean_train_loss))\n",
    "print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "162aa270-c30c-46b9-b039-3416bdcc34b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7234, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df15476-7e07-466f-b389-b97af11f19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting temperatures\n",
    "# given this segmentation, what temp are the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563701c-6cf0-470c-b269-351de2a09f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c04757-3af3-4ec5-86c0-702b53b371ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the test loss is greated than the train loss = overfit\n",
    "# can run the model on any of the test images from the test dataset - try running the model with a bunch of image patches and see if you think it's running sensible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcfdf3-1e6e-461b-b1bb-8bafd11dbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to modify from this for visualizing results: https://medium.com/@fernandopalominocobo/mastering-u-net-a-step-by-step-guide-to-segmentation-from-scratch-with-pytorch-6a17c5916114\n",
    "# didn't get very far\n",
    "def random_images_inference(inputs, targets, image_paths, model_pth, device):\n",
    "    model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_pth, map_location=torch.device(device)))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512))\n",
    "    ])\n",
    "\n",
    "    # Iterate for the images, masks and paths\n",
    "    for image_pth, mask_pth, image_paths in zip(image_tensors, mask_tensors, image_paths):\n",
    "        # Load the image\n",
    "        img = transform(image_pth)\n",
    "        \n",
    "        # Predict the imagen with the model\n",
    "        pred_mask = model(img.unsqueeze(0))\n",
    "        pred_mask = pred_mask.squeeze(0).permute(1,2,0)\n",
    "        \n",
    "        # Load the mask to compare\n",
    "        mask = transform(mask_pth).permute(1, 2, 0).to(device)\n",
    "        \n",
    "        print(f\"Image: {os.path.basename(image_paths)}, DICE coefficient: {round(float(dice_coefficient(pred_mask, mask)),5)}\")\n",
    "        \n",
    "        # Show the images\n",
    "        img = img.cpu().detach().permute(1, 2, 0)\n",
    "        pred_mask = pred_mask.cpu().detach()\n",
    "        pred_mask[pred_mask < 0] = 0\n",
    "        pred_mask[pred_mask > 0] = 1\n",
    "        \n",
    "        plt.figure(figsize=(15, 16))\n",
    "        plt.subplot(131), plt.imshow(imput), plt.title(\"original\")\n",
    "        plt.subplot(132), plt.imshow(target, cmap=\"gray\"), plt.title(\"predicted\")\n",
    "        plt.subplot(133), plt.imshow(pred, cmap=\"gray\"), plt.title(\"mask\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c7827-74a8-43c6-acb1-acd55007e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(\n",
    "#     mpl.image.imread('/Users/nben/Desktop/monthly images/6_25/RGB/FLIR2369.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4261935-6eca-4490-a7c6-fec5ae6a0b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bc588-d32f-4756-a448-de739ca8c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(th0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dc5fd-f02d-4a3f-b17e-428c3943081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00b96e-ab37-40e5-adfd-ca0672f078b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    np.stack(\n",
    "        [pcv.hist_equalization(im0[:,:,k])\n",
    "         for k in (0,1,2)],\n",
    "        axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25556d6f-1089-41a4-9d7d-9b3162a62e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pcv.threshold.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d5c94-a8d3-4263-9937-517d142957d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a081441-fe76-407b-ae38-0831565e7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_img = pcv.rgb2gray_lab(rgb_img=im, channel='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2b805-43aa-4218-bb66-bb071784b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(b_img, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b4719-3e37-4bed-bf89-c13b639a7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_mask = pcv.threshold.binary(gray_img=b_img, threshold=120, object_type='light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd272df-56c2-41b4-8489-4f0e7014fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thresh_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4c5d8-0912-488f-b5fa-40ef83ad1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "immask = np.array(im)\n",
    "immask[thresh_mask > 0, :] = 0\n",
    "plt.imshow(immask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc74a-f326-4006-91ff-e51b03c7f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_figure1, hist_data1 = pcv.visualize.histogram(img = b_img, hist_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056c237-8509-483f-b42d-61bb46bb270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_figure1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf6848-24e2-41a8-8113-4891d926c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = mpl.image.imread('/Users/nben/Desktop/FLIR3099.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839d93b-fce4-4a78-8972-9d9f6cd0f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
