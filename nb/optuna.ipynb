{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8bedc-c492-4025-8ab5-ac58dc841983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f7ea2d-8630-4092-8a63-3d9850d95de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os, warnings\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import skimage as ski\n",
    "import sklearn as skl\n",
    "\n",
    "#from plantcv import plantcv as pcv\n",
    "import flyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d804dedc-c26f-4eef-8053-70674ba05d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader class\n",
    "class FlirDataset(torch.utils.data.Dataset):\n",
    "    \"\"\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, k):\n",
    "        return self.samples[k]\n",
    "    def __init__(self, path, datatype='train', image_subsize=64, stride=None): \n",
    "        from glob import glob\n",
    "        from pathlib import Path\n",
    "        path = Path(path)\n",
    "        if datatype == \"train\":\n",
    "            tt = \"train\"\n",
    "        elif datatype == \"test\":\n",
    "            tt = \"test\"\n",
    "        elif datatype == \"validation\":\n",
    "            tt = \"validation\"\n",
    "        elif datatype is None:\n",
    "            tt = \"non\"\n",
    "        else:\n",
    "            raise ValueError(\"datatype must be 'train', 'test', 'validation', or None\")\n",
    "        annot_pattern = str(path / \"training\" / \"annotated\" / tt / \"*.png\")\n",
    "        annot_filenames = glob(annot_pattern)\n",
    "        annot_ims = {\n",
    "            Path(filename).name[:-4]: plt.imread(filename)\n",
    "            for filename in annot_filenames}\n",
    "        flir_pattern = str(path / \"*\" / \"thermal\" / \"*.jpg\")\n",
    "        flir_filenames = {\n",
    "            Path(file).name[:-4]: file\n",
    "            for file in glob(flir_pattern)}\n",
    "        self.names = list(annot_ims.keys())\n",
    "        flir_ims = {\n",
    "            key: self.load_flir(flir_filenames[key])\n",
    "            for key in self.names}\n",
    "        self.images = flir_ims\n",
    "        self.annots = annot_ims\n",
    "        #for (file, ims, annot) in zip(list(self.names()), list(self.images()), list(self.annots())):\n",
    "            #if ims[1].shape[:2] != annot.shape[:2]:\n",
    "               # plt.imshow(annot)\n",
    "                #print(file, ims[1].shape, annot.shape)\n",
    "        imsz = image_subsize\n",
    "        stride = imsz if stride is None else stride\n",
    "        sdata = {}\n",
    "        for key in self.names:\n",
    "            (thr_im, opt_im) = self.images[key]\n",
    "            ann_im = self.annots[key]\n",
    "            for (rno, rowidx) in enumerate(range(0, opt_im.shape[0], stride)):\n",
    "                if rowidx + imsz >= opt_im.shape[0]:\n",
    "                    continue\n",
    "                for (cno, colidx) in enumerate(range(0, opt_im.shape[1], stride)):\n",
    "                    if colidx + imsz >= opt_im.shape[1]:\n",
    "                        continue\n",
    "                    # Get the subimage from the optical and annotation images:\n",
    "                    opt_sub = opt_im[rowidx:rowidx + imsz, colidx:colidx + imsz]\n",
    "                    thr_sub = thr_im[rowidx:rowidx + imsz, colidx:colidx + imsz]\n",
    "                    ann_sub = ann_im[rowidx:rowidx + imsz, colidx:colidx + imsz]\n",
    "                    tup = (rowidx, colidx, opt_sub, ann_sub, thr_sub)\n",
    "                    sdata[key, rno, cno] = tup\n",
    "        self.sample_data = sdata\n",
    "        self.masks = {}\n",
    "        self.image_subsize = image_subsize\n",
    "        self.samples = []\n",
    "        for ((k,rno,cno), tup) in sdata.items():\n",
    "            (rowidx, colidx, opt_sub, ann_sub, thr_sub) = tup\n",
    "            #plant_pixels = np.all(ann_sub[:,:,0:3] == [1, 0, 0], axis=2)\n",
    "            plant_pixels = (ann_sub[:,:,0] - ann_sub[:,:,1] - ann_sub[:,:,2] > 0.9)\n",
    "            self.masks[k, rno, cno] = plant_pixels\n",
    "            opt_for_torch = torch.permute(\n",
    "                torch.tensor(opt_sub, dtype=torch.float) / 255,\n",
    "                (2, 0, 1))\n",
    "            ann_frac = 1 - np.sum(plant_pixels) / plant_pixels.size\n",
    "            #ann_frac = torch.tensor(\n",
    "            #    round(ann_frac * 999),\n",
    "            #    dtype=torch.long)\n",
    "            ann_frac = torch.tensor(ann_frac, dtype=torch.float)\n",
    "            #sample = (opt_for_torch, ann_frac)\n",
    "            mask = torch.tensor(self.masks[k, rno, cno], dtype=torch.float32)\n",
    "            sample = (opt_for_torch, mask[None,...])\n",
    "            self.samples.append(sample)\n",
    "    def load_flir(self, filename, thermal_unit='celsius'):\n",
    "        \"\"\"Loads and returns the portion of a FLIR image file that contains both\n",
    "        optical and thermal data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : pathlike\n",
    "            A ``pathname.Path`` object or a string representing the filename of\n",
    "            image that is to be loaded.\n",
    "        thermal_unit : {'celsius' | 'kelvin' | 'fahrenheit'}, optional\n",
    "            What temperature units to return; the default is ``'celsius'``.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        optical_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols, 3)`` containing the RGB\n",
    "            optical of the visual FLIR image.\n",
    "        thermal_image : numpy.ndarray\n",
    "            An image-array with shape ``(rows, cols)`` containing the thermal\n",
    "            values in Celsius.\n",
    "        \"\"\"\n",
    "        from os import fspath\n",
    "        from PIL import Image\n",
    "        import flyr\n",
    "        # Make sure we have a path:\n",
    "        filename = fspath(filename)\n",
    "        # Import the raw image data:\n",
    "        flir_image = flyr.unpack(filename)\n",
    "        # Extract the optical and thermal data:\n",
    "        opt = flir_image.optical\n",
    "        #plt.imshow(opt)\n",
    "        thr = getattr(flir_image, thermal_unit)\n",
    "        pip = flir_image.pip_info\n",
    "        x0 = pip.offset_x\n",
    "        y0 = pip.offset_y\n",
    "        ratio = pip.real_to_ir\n",
    "        ratio = opt.shape[0] / thr.shape[0] / ratio\n",
    "        # Resize the thermal image to match the optical image in resolution:\n",
    "        (opt_rs, opt_cs, _) = opt.shape\n",
    "        (thr_rs, thr_cs) = np.round(np.array(thr.shape) * ratio).astype(int)\n",
    "        thr = np.array(Image.fromarray(thr).resize([thr_cs, thr_rs]))\n",
    "        #plt.imshow(thr)\n",
    "        x0 = round(opt_cs // 2 - thr_cs // 2 + x0)\n",
    "        y0 = round(opt_rs // 2 - thr_rs // 2 + y0)\n",
    "        return (thr, opt[y0:y0+thr_rs, x0:x0+thr_cs, :])\n",
    "    def pred_all(self, model):\n",
    "        \"\"\"Returns predicted segmentations for all items in the dataset.\"\"\"\n",
    "        shape = (self.image_subsize, self.image_subsize)\n",
    "        inpts = torch.stack([img for (img,_) in self.samples if img.shape[1:] == shape], axis=0).detach()\n",
    "        targs = torch.stack([trg for (_,trg) in self.samples if trg.shape[1:] == shape], axis=0).detach()\n",
    "        preds = model(inpts).detach()\n",
    "        if model.logits:\n",
    "            preds = torch.sigmoid(preds)\n",
    "        return (\n",
    "            torch.permute(inpts, (0,2,3,1)),\n",
    "            preds[:, 0, ...],\n",
    "            torch.permute(targs, (0,2,3,1)))\n",
    "    def extract_temp(self, model):\n",
    "        \"\"\"Extracts temperature from predicted plant segmentation\"\"\"\n",
    "        results = []\n",
    "        for ((input, _), sdata) in zip(self.samples, self.sample_data.values()):\n",
    "            thermal_im = sdata[-1]\n",
    "            pred = model(input[None,...]) #need none bc model expecting batch, so gives it a batch dimension\n",
    "            # prediction is > 0.5 if pred is > 0 because pred is in logits and \n",
    "            # sigmoid() converts 0 to 0.5.\n",
    "            # So the line that follows is equivalent to:\n",
    "            # pred = torch.sigmoid(pred[0]) > 0.5\n",
    "            pred = pred[0, 0, ...] > 0\n",
    "            #print(pred.shape, type(thermal_im), input)\n",
    "            thermal_inseg = thermal_im[pred].flatten()\n",
    "            thermal_outseg = thermal_im[~pred].flatten()\n",
    "            results.append((thermal_inseg, thermal_outseg))\n",
    "        return results\n",
    "    def image_temps(self, model):\n",
    "        \"\"\"Gives us back images matched to temperatures\"\"\"\n",
    "        (_,preds,_) = self.pred_all(model)\n",
    "        result = {}\n",
    "        for (pred, ((file,r,c),sdata)) in zip(preds, self.sample_data.items()):\n",
    "            thermal = sdata[-1]\n",
    "            plant_temp = torch.sum(pred*thermal)\n",
    "            notpred = 1-pred\n",
    "            none_temp = torch.sum(notpred*thermal)\n",
    "            if file not in result: \n",
    "                result[file] = []\n",
    "            result[file].append((r, c, plant_temp, none_temp, pred, notpred))\n",
    "        df = []\n",
    "        for file, patches in result.items():\n",
    "            plant_temp = torch.sum(\n",
    "                torch.stack([t for (_,_,t,_,_,_) in patches]))\n",
    "            plant_temp /= torch.sum(\n",
    "                torch.stack([w for (_,_,_,_,w,_) in patches]))\n",
    "            none_temp = torch.sum(\n",
    "                torch.stack([t for (_,_,_,t,_,_) in patches]))\n",
    "            none_temp /= torch.sum(\n",
    "                torch.stack([w for (_,_,_,_,_,w) in patches]))\n",
    "            df.append(\n",
    "                {\"file\":file,\n",
    "                 \"plant_temp\": float(plant_temp.detach()),\n",
    "                 \"none_temp\":  float(none_temp.detach())})\n",
    "        return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b2f0c0-b79a-4deb-b6aa-f5f1063480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name the datasets\n",
    "dataset_path = \"/Users/ameliakeyser-gibson/Documents/SEFS/Kim Lab/vines grant/thermal/monthly images\"\n",
    "#dataset_path = \"/Users/nben/Documents/eScience/Accelerator/Vines-Thermal/monthly images\"\n",
    "#dataset_path = \"/data/vines-thermal/images\"\n",
    "image_subsize = 128\n",
    "stride = 32\n",
    "\n",
    "train_ds = FlirDataset(dataset_path, datatype=\"train\", image_subsize=image_subsize, stride=stride)\n",
    "test_ds = FlirDataset(dataset_path, datatype=\"test\", image_subsize=image_subsize)\n",
    "train_eval_ds = FlirDataset(dataset_path, datatype=\"train\", image_subsize=image_subsize)\n",
    "screc = FlirDataset(dataset_path, datatype=None, image_subsize=image_subsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e24c1f-caf5-476f-83b6-3ba6f6e731bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "#U-net code\n",
    "# Dependencies\n",
    "def convrelu(in_channels, out_channels,\n",
    "             kernel=3, padding=None, stride=1, bias=True, inplace=True):\n",
    "    \"\"\"Shortcut for creating a PyTorch 2D convolution followed by a ReLU.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        The number of input channels in the convolution.\n",
    "    out_channels : int\n",
    "        The number of output channels in the convolution.\n",
    "    kernel : int, optional\n",
    "        The kernel size for the convolution (default: 3).\n",
    "    padding : int or None, optional\n",
    "        The padding size for the convolution; if `None` (the default), then\n",
    "        chooses a padding size that attempts to maintain the image-size.\n",
    "    stride : int, optional\n",
    "        The stride to use in the convolution (default: 1).\n",
    "    bias : boolean, optional\n",
    "        Whether the convolution has a learnable bias (default: True).\n",
    "    inplace : boolean, optional\n",
    "        Whether to perform the ReLU operation in-place (default: True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Sequential\n",
    "        The model of a 2D-convolution followed by a ReLU operation.\n",
    "    \"\"\"\n",
    "    if padding is None:\n",
    "        padding = kernel_default_padding(kernel)\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, out_channels, kernel,\n",
    "                        padding=padding, bias=bias),\n",
    "        torch.nn.ReLU(inplace=inplace))\n",
    "\n",
    "#===============================================================================\n",
    "# Image-based CNN Model Code\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    \"\"\"a U-Net with a ResNet18 backbone for learning visual area labels.\n",
    "\n",
    "    The `UNet` class implements a [\"U-Net\"](https://arxiv.org/abs/1505.04597)\n",
    "    with a [ResNet-18](https://pytorch.org/hub/pytorch_vision_resnet/) bacbone.\n",
    "    The class inherits from `torch.nn.Module`.\n",
    "    \n",
    "    The original implementation of this class was by Shaoling Chen\n",
    "    (sc6995@nyu.edu), and additional modifications have been made by Noah C.\n",
    "    Benson (nben@uw.edu).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_count : int\n",
    "        The number of channels (features) in the input image. When using an\n",
    "        `HCPVisualDataset` object for training, this value should be set to 4\n",
    "        if the dataset uses the `'anat'` or `'func'` features and 8 if it uses\n",
    "        the `'both'` features.\n",
    "    segment_count : int\n",
    "        The number of segments (AKA classes, labels) in the output data. For\n",
    "        V1-V3 this is typically either 3 (V1, V2, V3) or 6 (LV1, LV2, LV3, RV1,\n",
    "        RV2, RV3).\n",
    "    base_model : model name or tuple, optional\n",
    "        The name of the model that is to be used as the base/backbone of the\n",
    "        UNet. The default is `'resnet18'`, but \n",
    "    pretrained : boolean, optional\n",
    "        Whether to use a pretrained base model for the backbone (`True`) or not\n",
    "        (`False`). The default is `False`.\n",
    "    logits : boolean, optional\n",
    "        Whether the model should return logits (`True`) or probabilities\n",
    "        (`False`). The default is `True`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pretrained_base : boolean\n",
    "        `True` if the base model used in this `UNet` was originally pre-trained\n",
    "        and `False` otherwise.\n",
    "    base_model : PyTorch Module\n",
    "        The ResNet-18 model that is used as the backbone of the `UNet` model.\n",
    "    base_layers : list of PyTorch Modules\n",
    "        The ResNet-18 layers that are used in the backbone of the `UNet` model.\n",
    "    feature_count : int\n",
    "        The number of input channels (features) that the model expects in input\n",
    "        images.\n",
    "    segment_count : int\n",
    "        The number of segments (labels) predicted by the model.\n",
    "    logits : bool\n",
    "        `True` if the output of the model is in logits and `False` if its output\n",
    "        is in probabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_count=3, segment_count=1,\n",
    "                 base_model='resnet18',\n",
    "                 pretrained=True,\n",
    "                 logits=True):\n",
    "        import torch.nn as nn\n",
    "        # Initialize the super-class.\n",
    "        super().__init__()\n",
    "        # Store some basic attributes.\n",
    "        self.feature_count = feature_count\n",
    "        self.segment_count = segment_count\n",
    "        self.pretrained = pretrained\n",
    "        self.logits = logits\n",
    "        # Set up the base model and base layers for the model.\n",
    "        if pretrained:\n",
    "            weights = 'IMAGENET1K_V1'\n",
    "        else:\n",
    "            weights = None\n",
    "        import torchvision.models as mdls\n",
    "        base_model = getattr(mdls, base_model)\n",
    "        try:\n",
    "            base_model = base_model(weights=weights)\n",
    "        except TypeError:\n",
    "            base_model = base_model(pretrained=pretrained)\n",
    "        # Not sure we should store the base model; seems like a good idea, but\n",
    "        # does it get caught up in PyTorch's Module data when we do?\n",
    "        #self.base_model = resnet18(pretrained=pretrained)\n",
    "        # Because the input size may not be 3 and the output size may not be 3,\n",
    "        # we want to add an additional \n",
    "        if feature_count != 3:\n",
    "            # Adjust the first convolution's number of input channels.\n",
    "            c1 = base_model.conv1\n",
    "            base_model.conv1 = nn.Conv2d(\n",
    "                feature_count, c1.out_channels,\n",
    "                kernel_size=c1.kernel_size, stride=c1.stride,\n",
    "                padding=c1.padding, bias=c1.bias)\n",
    "        base_layers = list(base_model.children())\n",
    "        #self.base_layers = base_layers\n",
    "        # Make the U-Net layers out of the base-layers.\n",
    "        # size = (N, 64, H/2, W/2)\n",
    "        self.layer0 = nn.Sequential(*base_layers[:3]) \n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 64, H/4, W/4)\n",
    "        self.layer1 = nn.Sequential(*base_layers[3:5])\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        # size = (N, 128, H/8, W/8)        \n",
    "        self.layer2 = base_layers[5]\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        # size = (N, 256, H/16, W/16)\n",
    "        self.layer3 = base_layers[6]  \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        # size = (N, 512, H/32, W/32)\n",
    "        self.layer4 = base_layers[7]\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "        # The up-swing of the UNet; we will need to upsample the image.\n",
    "        self.upsample = nn.Upsample(scale_factor=2,\n",
    "                                    mode='bilinear',\n",
    "                                    align_corners=True)\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        self.conv_original_size0 = convrelu(feature_count, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        self.conv_last = nn.Conv2d(64, segment_count, 1)\n",
    "    def forward(self, input):\n",
    "        # Do the original size convolutions.\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        # Now the front few layers, which we save for adding back in on the UNet\n",
    "        # up-swing below.\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        # Now, we start the up-swing; each step must upsample the image.\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        # Up-swing Step 1\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        # Up-swing Step 2\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        # Up-swing Step 3\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "        # Up-swing Step 4\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        # Up-swing Step 5\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        # And the final convolution.\n",
    "        out = self.conv_last(x)\n",
    "        if not self.logits:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501743ce-d474-4e0d-8805-c9fa4f4cb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used in dice coefficient function\n",
    "def is_logits(data): \n",
    "    \"\"\"Attempts to guess whether the given PyTorch tensor contains logits.\n",
    "\n",
    "    If the argument `data` contains only values that are no less than 0 and no\n",
    "    greater than 1, then `False` is returned; otherwise, `True` is returned.\n",
    "    \"\"\"\n",
    "    if   (data > 1).any(): return True\n",
    "    elif (data < 0).any(): return True\n",
    "    else:                  return False\n",
    "# writing a test DICE loss coefficient        \n",
    "def dice_loss(pred, gold, logits=None, smoothing=0, metrics=None):\n",
    "    \"\"\"Returns the loss based on the dice coefficient.\n",
    "    \n",
    "    `dice_loss(pred, gold)` returns the dice-coefficient loss between the\n",
    "    tensors `pred` and `gold` which must be the same shape and which should\n",
    "    represent probabilities. The first two dimensions of both `pred` and `gold`\n",
    "    must represent the batch-size and the classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : tensor\n",
    "        The predicted probabilities of each class.\n",
    "    gold : tensor\n",
    "        The gold-standard labels for each class.\n",
    "    logits : boolean, optional\n",
    "        Whether the values in `pred` are logits--i.e., unnormalized scores that\n",
    "        have not been run through a sigmoid calculation already. If this is\n",
    "        `True`, then the BCE starts by calculating the sigmoid of the `pred`\n",
    "        argument. If `None`, then attempts to deduce whether the input is or is\n",
    "        not logits. The default is `None`.\n",
    "    smoothing : number, optional\n",
    "        The smoothing coefficient `s`. The default is `1`.\n",
    "    metrics : dict or None, optional\n",
    "        An optional dictionary into which the key `'dice'` should be inserted\n",
    "        with the dice-loss as the value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The dice-coefficient loss of the prediction.\n",
    "    \"\"\"\n",
    "    pred = pred.contiguous()\n",
    "    gold = gold.contiguous()\n",
    "    if logits is None: logits = is_logits(pred) #sometimes logit, sometimes probability, this func automatically detect whether logit or prob\n",
    "    if logits: pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * gold) #high probabilities get higher values, low get low, gold is 0s and 1s, this gives predicted probabilities where true value is correct\n",
    "    pred = pred**2 #noah checking if we should be squaring here\n",
    "    gold = gold**2\n",
    "    while len(intersection.shape) > 2:\n",
    "        intersection = intersection.sum(dim=-1)\n",
    "        pred = pred.sum(dim=-1)\n",
    "        gold = gold.sum(dim=-1)\n",
    "    if smoothing is None: smoothing = 0\n",
    "    loss = (1 - ((2 * intersection + smoothing) / (pred + gold + smoothing)))\n",
    "    # Average the loss across classes then take the mean across batch elements.\n",
    "    loss = loss.mean(dim=1).mean() #utilities that we can ignore - mean across channels and then across all the batches\n",
    "    if metrics is not None:\n",
    "        if 'dice' not in metrics: metrics['dice'] = 0.0\n",
    "        metrics['dice'] += loss.data.cpu().numpy() * gold.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b1cc1-ec14-40ee-b223-bc7e5cb6ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lr, gamma, batch_size, n_epochs=10, base_model='resnet18', print=print):\n",
    "    if print is None: \n",
    "        print = lambda *args,**kw: None\n",
    "    print(\"Training Model...\")\n",
    "    print(f\"  lr    = {lr}\")\n",
    "    print(f\"  gamma = {gamma}\")\n",
    "    print(f\"  bsize = {batch_size}\")\n",
    "    \n",
    "    model = UNet(base_model=base_model)\n",
    "    \n",
    "    # Make the optimizer and LR-manager:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer handles updating the parameters each step\n",
    "    steplr = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=1,\n",
    "        gamma=gamma) #take smaller steps in the learning rate as you get closer\n",
    "    \n",
    "    # Declare our loss function: what's actually getting minimized\n",
    "    bce_loss_fn = torch.nn.BCEWithLogitsLoss()  #loss function that works for pixels and logits- well established\n",
    "    dice_loss_fn = lambda a, b: dice_loss(a, b, smoothing=0, logits=True)\n",
    "    both_loss_fn = lambda a,b,w=0.5: (1-w)*bce_loss_fn(a,b) + w*dice_loss_fn(a,b)\n",
    "    \n",
    "    \n",
    "    # Make the dataloaders:\n",
    "    train_dloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "    test_dloader = torch.utils.data.DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=len(test_ds),\n",
    "        shuffle=False)\n",
    "    train_eval_dloader = torch.utils.data.DataLoader(\n",
    "        train_eval_ds,\n",
    "        batch_size=len(train_eval_ds),\n",
    "        shuffle=False)\n",
    "    \n",
    "    # Now we start the optimization loop:\n",
    "    for epoch_num in range(n_epochs):\n",
    "        print(f\"    * Epoch: {epoch_num}\")\n",
    "        #loss_fn = lambda a,b: both_loss_fn(a, b, w=(epoch_num + 1)/ n_epochs)\n",
    "        loss_fn = both_loss_fn\n",
    "        # Put the model in train mode:\n",
    "        model.train()\n",
    "        # In each epoch, we go through each training sample once; the dataloader\n",
    "        # gives these to us in batches:\n",
    "        total_train_loss = 0\n",
    "        for (inputs, targets) in train_dloader:\n",
    "            # We're starting a new step, so we reset the gradients.\n",
    "            optimizer.zero_grad()\n",
    "            # Calculate the model prediction for these inputs.\n",
    "            preds = model(inputs)\n",
    "            # Calculate the loss between the prediction and the actual outputs.\n",
    "            train_loss = loss_fn(preds, targets) #sigmoid gives the probability but don't need sigmoid bc of BCE w/logit loss\n",
    "            # Have PyTorch backward-propagate the gradients.\n",
    "            train_loss.backward()\n",
    "            # Have the optimizer take a step: (update the parameters)\n",
    "            optimizer.step()\n",
    "            # Add up the total training loss:\n",
    "            total_train_loss += float(train_loss.detach())*len(targets)\n",
    "            train_loss = None\n",
    "        # LR Scheduler step:\n",
    "        steplr.step() #make the learning rate smaller\n",
    "        mean_train_loss = total_train_loss / len(train_ds)\n",
    "        if not np.isfinite(mean_train_loss):\n",
    "            return (model, np.nan)\n",
    "        # Now that we've finished training, put the model back in evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            ## Evaluate the model using the test data.\n",
    "            total_test_dice_loss = 0\n",
    "            total_test_bce_loss = 0\n",
    "            total_test_loss = 0\n",
    "            for (inputs, targets) in test_dloader:\n",
    "                preds = model(inputs)\n",
    "                test_loss = loss_fn(preds, targets)\n",
    "                total_test_loss += float(test_loss.detach()) * len(targets) # changed from train loss\n",
    "                total_test_dice_loss += float(dice_loss_fn(preds, targets).detach()) * len(targets)\n",
    "                total_test_bce_loss += float(bce_loss_fn(preds, targets).detach()) * len(targets)\n",
    "            mean_test_loss = total_test_loss / len(test_ds)\n",
    "            mean_test_dice_loss = total_test_dice_loss / len(test_ds)\n",
    "            mean_test_bce_loss = total_test_bce_loss / len(test_ds)\n",
    "            total_train_dice_loss = 0\n",
    "            total_train_bce_loss = 0\n",
    "            for (inputs, targets) in train_eval_dloader:\n",
    "                preds = model(inputs)\n",
    "                total_train_dice_loss += float(dice_loss_fn(preds, targets).detach()) * len(targets)\n",
    "                total_train_bce_loss += float(bce_loss_fn(preds, targets).detach()) * len(targets)\n",
    "            mean_train_dice_loss = total_train_dice_loss / len(train_eval_ds)\n",
    "            mean_train_bce_loss = total_train_bce_loss / len(train_eval_ds)\n",
    "        # Print something about this step:\n",
    "        print(\n",
    "            f\"      train loss: \"\n",
    "            f\"{mean_train_loss:6.3f} [{mean_train_dice_loss:6.3f} {mean_train_bce_loss:6.3f}]\")\n",
    "        print(\n",
    "            f\"      test loss: \"\n",
    "            f\"{mean_test_loss:6.3f} [{mean_test_dice_loss:6.3f} {mean_test_bce_loss:6.3f}]\")\n",
    "    # After the optimizer has run, print out what it's found:\n",
    "    print(\"Final result:\")\n",
    "    print(f\"  train dice loss = \", float(mean_train_dice_loss))\n",
    "    print(f\"  test dice loss = \", float(mean_test_dice_loss))\n",
    "    return (model, float(mean_test_dice_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82648d-ccae-4edb-a31d-a2f3c73b58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somewhere else is our function train_model(lr, gamma, batch_size);\n",
    "# it should train and return a model.\n",
    "# The return value should be a tuple (model, loss) where loss is the\n",
    "# validation dataset loss for the trained model.\n",
    "\n",
    "# The train_ds needs to be defined before the code below gets run.\n",
    "runno = 1\n",
    "data_path = Path(f'/data/vines-thermal/optuna/run{runno}')\n",
    "model_save_path = data_path / 'models'\n",
    "study_db_path = data_path / 'study.db'\n",
    "study_name = 'vines-thermal'\n",
    "\n",
    "if not data_path.exists():\n",
    "    data_path.mkdir(mode=0o775, exist_ok=True, parents=True)\n",
    "if not model_save_path.exists():\n",
    "    model_save_path.mkdir(mode=0o775, exist_ok=True, parents=True)\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 0.00001, 0.0025)\n",
    "    gamma = trial.suggest_float('gamma', 0.25, 1)\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 1024)\n",
    "    (model, loss) = train_model(lr, gamma, batch_size)\n",
    "    if model_save_path is not None:\n",
    "        filename = f\"optuna_lr-{lr}_gamma-{gamma}_bs-{batch_size}.pt\"\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            model_save_path / filename)\n",
    "    return loss\n",
    "\n",
    "study_db = f'sqlite:///{study_db_path}'\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=study_db,\n",
    "    load_if_exists=True)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "savedata = dict(study.best_params, best_dice=study.best_value)\n",
    "with (data_path / 'best_params.json').open('w') as file:\n",
    "    json.dump(savedata, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
